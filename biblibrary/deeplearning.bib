@inproceedings{10.5555/2969442.2969628,
  title = {Learning Structured Output Representation Using Deep Conditional Generative Models},
  booktitle = {Proceedings of the 29th International Conference on Neural Information Processing Systems - Volume 2},
  author = {Sohn, Kihyuk and Yan, Xinchen and Lee, Honglak},
  date = {2015},
  series = {{{NIPS}}'15},
  pages = {3483--3491},
  publisher = {MIT Press},
  location = {Cambridge, MA, USA},
  pagetotal = {9},
  annotation = {titleTranslation: 利用深度条件生成模型学习结构化输出表示\\
abstractTranslation:  有监督的深度学习已经成功应用于许多识别问题。虽然当提供大量的训练数据时，它可以很好地近似复杂的多对一函数，但对复杂的结构化输出表示进行建模，从而有效地进行概率推理并做出多样化的预测仍然具有挑战性。在这项工作中，我们开发了一个使用高斯潜变量的结构化输出预测的深度条件生成模型。该模型在随机梯度变分贝叶斯框架下进行高效训练，并允许使用随机前馈推断进行快速预测。此外，我们提供了新的策略来构建鲁棒的结构化预测算法，例如输入噪声注入和多尺度预测目标化}
}

@inproceedings{10.5555/3295222.3295349,
  title = {Attention Is All You Need},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
  date = {2017},
  series = {{{NIPS}}'17},
  pages = {6000--6010},
  publisher = {Curran Associates Inc.},
  location = {Red Hook, NY, USA},
  isbn = {978-1-5108-6096-4},
  pagetotal = {11},
  annotation = {titleTranslation: 注意力是你所需要的一切\\
abstractTranslation:  占主导地位的序列转导模型是基于复杂的循环或卷积神经网络，包括编码器和解码器。表现最好的模型还通过注意力机制将编码器和解码器连接起来。我们提出了一种新的简单的网络结构，Transformer，它完全基于注意力机制，不需要递归和卷积。在两个机器翻译任务上的实验表明，这些模型在质量上更胜一筹，同时具有更高的可并行性，需要的训练时间也显著减少。我们的模型在WMT 2014英德翻译任务上达到了28.4 BLEU，比现有的最好结果(包括集成)提高了2 BLEU以上。在WMT 2014英法翻译任务上，我们的模型建立了一个}
}

@inproceedings{agarwalComprehensiveReviewLinear2024,
  title = {A {{Comprehensive Review}} of {{Linear Regression}}, {{Random Forest}}, {{XGBoost}}, and {{SVR}}: {{Integrating Machine Learning}} and {{Actuarial Science}} for {{Health Insurance Pricing}}},
  booktitle = {Data {{Science}} and {{Security}}},
  author = {Agarwal, Vedant and Singh, Mehakdeep and Kumar, Kukatlapalli Pradeep},
  date = {2024},
  pages = {355--367},
  publisher = {Springer Nature Singapore},
  location = {Singapore},
  isbn = {978-981-9709-75-5},
  annotation = {titleTranslation: 全面回顾了线性回归、随机森林、Xgboost和Svr：整合机器学习和精算学进行健康险定价\\
abstractTranslation:  精算科学和数据科学正在被研究作为使用物联网、人工智能、大数据和机器学习( ML )算法等工业4.0技术的融合。当分析精算学的早期成分时，本来可以更加准确和快速，但当AI和ML的后期阶段被整合时，算法并没有达到标准，精算师也经历了一些准确性的担忧。公司要求精算师在分析中做到精确，以获得可靠的结果。由于这些公司收集了大量的数据，手动做出的选择可能是错误的。因此，我们将在本文中检验替代模型，作为决策过程的一部分。一旦我们选择了最佳的行动路径，}
}

@incollection{bruntonMachineLearningFluid2020a,
  title = {Machine {{Learning}} for {{Fluid Mechanics}}},
  booktitle = {{{ANNUAL REVIEW OF FLUID MECHANICS}}, {{VOL}} 52},
  author = {family=Brunton, given=SL, given-i=SL and family=Noack, given=BR, given-i=BR and Koumoutsakos, P},
  date = {2020},
  volume = {52},
  pages = {477--508},
  isbn = {0066-4189},
  keywords = {ARTIFICIAL NEURAL-NETWORKS,control,DATA-DRIVEN,data-driven modeling,EVOLUTIONARY ALGORITHMS,FLOWS,IDENTIFICATION,machine learning,MODEL,optimization,OPTIMIZATION,SENSOR PLACEMENT,SYSTEMS,TURBULENCE},
  annotation = {titleTranslation: 面向流体力学的机器学习\\
abstractTranslation:  流体力学领域在多个时空尺度的实验、现场测量和大规模模拟的前所未有的数据量的推动下迅速发展。机器学习( ML )提供了丰富的技术来从数据中提取信息，这些信息可以转化为关于底层流体力学的知识。此外，ML算法可以增加领域知识，并自动执行与流控制和优化相关的任务。本文概述了流体力学ML的过去历史、当前发展和新出现的机会。我们概述了基本的ML方法，并讨论了它们在理解、建模、优化和控制流体流动方面的用途。这些方法的长处和局限从正反两方面得到说明}
}

@inproceedings{choLearningPhraseRepresentations2014,
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder}}–{{Decoder}} for {{Statistical Machine Translation}}},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Cho, Kyunghyun and Van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  date = {2014},
  pages = {1724--1734},
  publisher = {Association for Computational Linguistics},
  location = {Doha, Qatar},
  urldate = {2024-12-30},
  eventtitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  annotation = {titleTranslation: 利用RNN编码器-解码器学习短语表示进行统计机器翻译}
}

@article{elmanFindingStructureTime1990,
  title = {Finding {{Structure}} in {{Time}}},
  author = {Elman, Jeffrey L.},
  date = {1990-03},
  journaltitle = {Cognitive Science},
  shortjournal = {Cognitive Science},
  volume = {14},
  number = {2},
  pages = {179--211},
  urldate = {2024-12-30},
  annotation = {titleTranslation: 及时寻找结构，RNN原始文献\\
abstractTranslation:  时间是许多有趣的人类行为的基础。因此，如何在联结主义模型中表示时间的问题是非常重要的。一种方法是通过时间对加工的影响来隐式地表示时间，而不是显式地表示(作为一种空间表象)。本报告沿这些路线提出了一个建议，首先由Jordan ( 1986 )描述，其中涉及使用循环链接，以便为网络提供动态记忆。在这种方法中，隐藏的单元模式被反馈到自身：在先前的内部状态的背景下发展起来的内部表示，从而反映了任务需求。本文报告了一系列的模拟，从相对简单的问题( XOR的时态版本)到发现单词的句法/语义特征。网络ar}
}

@article{galtonRegressionMediocrityHereditary1886,
  title = {Regression towards Mediocrity in Hereditary Stature.},
  author = {Galton, Francis},
  date = {1886},
  journaltitle = {The Journal of the Anthropological Institute of Great Britain and Ireland},
  shortjournal = {The Journal of the Anthropological Institute of Great Britain and Ireland},
  volume = {15},
  pages = {246--263},
  publisher = {JSTOR},
  annotation = {titleTranslation: 在遗传性身材上向平庸回归。}
}

@book{Goodfellow-et-al-2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  date = {2016},
  publisher = {MIT Press},
  annotation = {titleTranslation: 深度学习}
}

@article{karlMultiObjectiveHyperparameterOptimization2023,
  title = {Multi-{{Objective Hyperparameter Optimization}} in {{Machine Learning}}—{{An Overview}}},
  author = {Karl, Florian and Pielok, Tobias and Moosbauer, Julia and Pfisterer, Florian and Coors, Stefan and Binder, Martin and Schneider, Lennart and Thomas, Janek and Richter, Jakob and Lang, Michel and Garrido-Merchán, Eduardo C. and Branke, Juergen and Bischl, Bernd},
  date = {2023-12-31},
  journaltitle = {ACM Transactions on Evolutionary Learning and Optimization},
  shortjournal = {ACM Trans. Evol. Learn. Optim.},
  volume = {3},
  number = {4},
  pages = {1--50},
  urldate = {2025-01-12},
  annotation = {abstractTranslation:  超参数优化是典型的现代机器学习( ML )工作流的重要组成部分。这是因为ML方法和相应的预处理步骤往往只有在适当调整超参数时才能获得最优的性能。但在许多应用中，我们不仅仅只对优化ML流水线以提高预测精度感兴趣；在确定最优配置时，必须考虑额外的度量或约束，从而导致多目标优化问题。由于缺乏多目标超参数优化的知识和容易获得的软件实现，这在实际中常常被忽视。在这项工作中，我们向读者介绍了多目标超参数优化的基础知识，并激励了它的u\\
titleTranslation: 机器学习中的多目标超参数优化- -综述}
}

@article{xuSmallDataMachine2023,
  title = {Small Data Machine Learning in Materials Science},
  author = {family=Xu, given=PC, given-i=PC and family=Ji, given=XB, given-i=XB and family=Li, given=MJ, given-i=MJ and family=Lu, given=WC, given-i=WC},
  date = {2023-03-25},
  journaltitle = {NPJ COMPUTATIONAL MATERIALS},
  volume = {9},
  number = {1},
  keywords = {DESIGN,DISCOVERY,DRIVEN,METHODOLOGIES,PLATFORM,RECOGNITION},
  annotation = {abstractTranslation:  本综述讨论了材料机器学习面临的小数据困境。首先，我们分析了小数据带来的局限性。然后，介绍了材料机器学习的工作流程。接下来介绍了处理小数据的方法，包括从出版物中提取数据、构建材料数据库、高通量计算以及从数据源层面进行实验；从算法层面对小数据和不平衡学习的算法进行建模；从机器学习策略层面进行主动学习和迁移学习。最后，提出了小数据机器学习在材料科学领域的未来发展方向。\\
titleTranslation: 材料科学中的小数据机器学习}
}
